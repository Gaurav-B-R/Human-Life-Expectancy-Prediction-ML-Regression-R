{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4709271",
      "metadata": {
        "origin_pos": 1,
        "id": "f4709271"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "Let's think briefly about what we\n",
        "expect from a good predictive model.\n",
        "We want it to peform well on unseen data.\n",
        "Classical generalization theory\n",
        "suggests that to close the gap between\n",
        "train and test performance,\n",
        "we should aim for a simple model.\n",
        "Simplicity can come in the form\n",
        "of a small number of dimensions.\n",
        "We explored this when discussing the\n",
        "monomial basis functions of linear models.\n",
        "Additionally, as we saw when discussing weight decay\n",
        "($\\ell_2$ regularization),\n",
        "the (inverse) norm of the parameters also\n",
        "represents a useful measure of simplicity.\n",
        "Another useful notion of simplicity is smoothness,\n",
        "i.e., that the function should not be sensitive\n",
        "to small changes to its inputs.\n",
        "For instance, when we classify images,\n",
        "we would expect that adding some random noise\n",
        "to the pixels should be mostly harmless.\n",
        "\n",
        "Scientists formalized\n",
        "this idea when he proved that training with input noise\n",
        "is equivalent to Tikhonov regularization.\n",
        "This work drew a clear mathematical connection\n",
        "between the requirement that a function be smooth (and thus simple),\n",
        "and the requirement that it be resilient\n",
        "to perturbations in the input.\n",
        "\n",
        "Then, :citet:`Srivastava.Hinton.Krizhevsky.ea.2014`\n",
        "developed a clever idea for how to apply Bishop's idea\n",
        "to the internal layers of a network, too.\n",
        "Their idea, called *dropout*, involves\n",
        "injecting noise while computing\n",
        "each internal layer during forward propagation,\n",
        "and it has become a standard technique\n",
        "for training neural networks.\n",
        "The method is called *dropout* because we literally\n",
        "*drop out* some neurons during training.\n",
        "Throughout training, on each iteration,\n",
        "standard dropout consists of zeroing out\n",
        "some fraction of the nodes in each layer\n",
        "before calculating the subsequent layer.\n",
        "\n",
        "To be clear, we are imposing\n",
        "our own narrative with the link to Bishop.\n",
        "The original paper on dropout\n",
        "offers intuition through a surprising\n",
        "analogy to sexual reproduction.\n",
        "The authors argue that neural network overfitting\n",
        "is characterized by a state in which\n",
        "each layer relies on a specific\n",
        "pattern of activations in the previous layer,\n",
        "calling this condition *co-adaptation*.\n",
        "Dropout, they claim, breaks up co-adaptation\n",
        "just as sexual reproduction is argued to\n",
        "break up co-adapted genes.\n",
        "While such an justification of this theory is certainly up for debate,\n",
        "the dropout technique itself has proved enduring,\n",
        "and various forms of dropout are implemented\n",
        "in most deep learning libraries.\n",
        "\n",
        "\n",
        "The key challenge is how to inject this noise.\n",
        "One idea is to inject it in an *unbiased* manner\n",
        "so that the expected value of each layer---while fixing\n",
        "the others---equals the value it would have taken absent noise.\n",
        "In Bishop's work, he added Gaussian noise\n",
        "to the inputs to a linear model.\n",
        "At each training iteration, he added noise\n",
        "sampled from a distribution with mean zero\n",
        "$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to the input $\\mathbf{x}$,\n",
        "yielding a perturbed point $\\mathbf{x}' = \\mathbf{x} + \\epsilon$.\n",
        "In expectation, $E[\\mathbf{x}'] = \\mathbf{x}$.\n",
        "\n",
        "In standard dropout regularization,\n",
        "one zeros out some fraction of the nodes in each layer\n",
        "and then *debiases* each layer by normalizing\n",
        "by the fraction of nodes that were retained (not dropped out).\n",
        "In other words,\n",
        "with *dropout probability* $p$,\n",
        "each intermediate activation $h$ is replaced by\n",
        "a random variable $h'$ as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h' =\n",
        "\\begin{cases}\n",
        "    0 & \\textrm{ with probability } p \\\\\n",
        "    \\frac{h}{1-p} & \\textrm{ otherwise}\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "By design, the expectation remains unchanged, i.e., $E[h'] = h$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "54feb90c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:10.343768Z",
          "iopub.status.busy": "2023-08-18T19:33:10.343431Z",
          "iopub.status.idle": "2023-08-18T19:33:13.572271Z",
          "shell.execute_reply": "2023-08-18T19:33:13.571155Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "54feb90c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a048453",
      "metadata": {
        "origin_pos": 6,
        "id": "4a048453"
      },
      "source": [
        "## Dropout in Practice\n",
        "\n",
        "Recall the MLP with a hidden layer and five hidden units\n",
        "from :numref:`fig_mlp`.\n",
        "When we apply dropout to a hidden layer,\n",
        "zeroing out each hidden unit with probability $p$,\n",
        "the result can be viewed as a network\n",
        "containing only a subset of the original neurons.\n",
        "In :numref:`fig_dropout2`, $h_2$ and $h_5$ are removed.\n",
        "Consequently, the calculation of the outputs\n",
        "no longer depends on $h_2$ or $h_5$\n",
        "and their respective gradient also vanishes\n",
        "when performing backpropagation.\n",
        "In this way, the calculation of the output layer\n",
        "cannot be overly dependent on any\n",
        "one element of $h_1, \\ldots, h_5$.\n",
        "\n",
        "![MLP before and after dropout.](http://d2l.ai/_images/dropout2.svg)\n",
        ":label:`fig_dropout2`\n",
        "\n",
        "Typically, we disable dropout at test time.\n",
        "Given a trained model and a new example,\n",
        "we do not drop out any nodes\n",
        "and thus do not need to normalize.\n",
        "However, there are some exceptions:\n",
        "some researchers use dropout at test time as a heuristic\n",
        "for estimating the *uncertainty* of neural network predictions:\n",
        "if the predictions agree across many different dropout outputs,\n",
        "then we might say that the network is more confident.\n",
        "\n",
        "## Implementation from Scratch\n",
        "\n",
        "To implement the dropout function for a single layer,\n",
        "we must draw as many samples\n",
        "from a Bernoulli (binary) random variable\n",
        "as our layer has dimensions,\n",
        "where the random variable takes value $1$ (keep)\n",
        "with probability $1-p$ and $0$ (drop) with probability $p$.\n",
        "One easy way to implement this is to first draw samples\n",
        "from the uniform distribution $U[0, 1]$.\n",
        "Then we can keep those nodes for which the corresponding\n",
        "sample is greater than $p$, dropping the rest.\n",
        "\n",
        "In the following code, we (**implement a `dropout_layer` function\n",
        "that drops out the elements in the tensor input `X`\n",
        "with probability `dropout`**),\n",
        "rescaling the remainder as described above:\n",
        "dividing the survivors by `1.0-dropout`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dcda4b93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.577007Z",
          "iopub.status.busy": "2023-08-18T19:33:13.576588Z",
          "iopub.status.idle": "2023-08-18T19:33:13.582474Z",
          "shell.execute_reply": "2023-08-18T19:33:13.581632Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "dcda4b93"
      },
      "outputs": [],
      "source": [
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    if dropout == 1: return torch.zeros_like(X)\n",
        "    mask = (torch.rand(X.shape) > dropout).float()\n",
        "    return mask * X / (1.0 - dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0d5f19",
      "metadata": {
        "origin_pos": 11,
        "id": "7a0d5f19"
      },
      "source": [
        "We can [**test out the `dropout_layer` function on a few examples**].\n",
        "In the following lines of code,\n",
        "we pass our input `X` through the dropout operation,\n",
        "with probabilities 0, 0.5, and 1, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1effb931",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.587061Z",
          "iopub.status.busy": "2023-08-18T19:33:13.586226Z",
          "iopub.status.idle": "2023-08-18T19:33:13.614970Z",
          "shell.execute_reply": "2023-08-18T19:33:13.614053Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "1effb931",
        "outputId": "f78e3d85-f3f3-4625-8964-9c721f761bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dropout_p = 0: tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
            "dropout_p = 0.5: tensor([[ 0.,  0.,  0.,  0.,  0.,  0., 12.,  0.],\n",
            "        [ 0.,  0., 20.,  0., 24.,  0., 28., 30.]])\n",
            "dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
        "print('dropout_p = 0:', dropout_layer(X, 0))\n",
        "print('dropout_p = 0.5:', dropout_layer(X, 0.5))\n",
        "print('dropout_p = 1:', dropout_layer(X, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "087cdc90",
      "metadata": {
        "origin_pos": 13,
        "id": "087cdc90"
      },
      "source": [
        "### Defining the Model\n",
        "\n",
        "The model below applies dropout to the output\n",
        "of each hidden layer (following the activation function).\n",
        "We can set dropout probabilities for each layer separately.\n",
        "A common choice is to set\n",
        "a lower dropout probability closer to the input layer.\n",
        "We ensure that dropout is only active during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a98d0264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.618877Z",
          "iopub.status.busy": "2023-08-18T19:33:13.618261Z",
          "iopub.status.idle": "2023-08-18T19:33:13.626219Z",
          "shell.execute_reply": "2023-08-18T19:33:13.625088Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "a98d0264"
      },
      "outputs": [],
      "source": [
        "class DropoutMLPScratch(torch.nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLPScratch, self).__init__()\n",
        "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
        "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
        "        self.lin3 = nn.LazyLinear(num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_1 = dropout_1\n",
        "        self.dropout_2 = dropout_2\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return self.lin3(H2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2793a6d2",
      "metadata": {
        "origin_pos": 18,
        "id": "2793a6d2"
      },
      "source": [
        "### Training\n",
        "\n",
        "Write your code to train the provided network similar to the training of MLPs described early in the lectures on the FashionMNIST Dataset for 10 epochs.\n",
        "\n",
        "https://pytorch.org/vision/0.19/generated/torchvision.datasets.FashionMNIST.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "12f6e01f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.630286Z",
          "iopub.status.busy": "2023-08-18T19:33:13.629522Z",
          "iopub.status.idle": "2023-08-18T19:34:13.198615Z",
          "shell.execute_reply": "2023-08-18T19:34:13.197238Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "12f6e01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19dce87-895a-444c-cff1-298e5a0595e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7891, Test Accuracy: 0.8097\n",
            "Epoch 2/10, Loss: 0.5210, Test Accuracy: 0.8207\n",
            "Epoch 3/10, Loss: 0.4715, Test Accuracy: 0.8396\n",
            "Epoch 4/10, Loss: 0.4449, Test Accuracy: 0.8508\n",
            "Epoch 5/10, Loss: 0.4234, Test Accuracy: 0.8548\n",
            "Epoch 6/10, Loss: 0.4078, Test Accuracy: 0.8560\n",
            "Epoch 7/10, Loss: 0.3954, Test Accuracy: 0.8578\n",
            "Epoch 8/10, Loss: 0.3859, Test Accuracy: 0.8643\n",
            "Epoch 9/10, Loss: 0.3756, Test Accuracy: 0.8656\n",
            "Epoch 10/10, Loss: 0.3694, Test Accuracy: 0.8633\n"
          ]
        }
      ],
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
        "model = DropoutMLPScratch(**hparams)\n",
        "\n",
        "#write your training and testing code here\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Loading the FashionMNIST dataset\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Defining training and evaluation functions\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Setting device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training setup\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=hparams['lr'])\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    test_accuracy = evaluate_model(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101c89c1",
      "metadata": {
        "origin_pos": 20,
        "id": "101c89c1"
      },
      "source": [
        "## Higher Level Implementation\n",
        "\n",
        "With high-level APIs, all we need to do is add a `Dropout` layer\n",
        "after each fully connected layer,\n",
        "passing in the dropout probability\n",
        "as the only argument to its constructor.\n",
        "During training, the `Dropout` layer will randomly\n",
        "drop out outputs of the previous layer\n",
        "(or equivalently, the inputs to the subsequent layer)\n",
        "according to the specified dropout probability.\n",
        "When not in training mode,\n",
        "the `Dropout` layer simply passes the data through during testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "224bafde",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:34:13.202812Z",
          "iopub.status.busy": "2023-08-18T19:34:13.202080Z",
          "iopub.status.idle": "2023-08-18T19:34:13.208377Z",
          "shell.execute_reply": "2023-08-18T19:34:13.207307Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "224bafde"
      },
      "outputs": [],
      "source": [
        "class DropoutMLP(torch.nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))\n",
        "    def forward(self, X):\n",
        "        return self.net(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877d8ec2",
      "metadata": {
        "origin_pos": 27,
        "id": "877d8ec2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Next, write your code to train the given model on the FashionMNIST Dataset for 10 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d9e0ea94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:34:13.212381Z",
          "iopub.status.busy": "2023-08-18T19:34:13.211782Z",
          "iopub.status.idle": "2023-08-18T19:35:25.030389Z",
          "shell.execute_reply": "2023-08-18T19:35:25.029011Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "d9e0ea94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5202375b-0c7f-473d-eb23-437de469e0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7909, Test Accuracy: 0.8090\n",
            "Epoch 2/10, Loss: 0.5210, Test Accuracy: 0.8334\n",
            "Epoch 3/10, Loss: 0.4704, Test Accuracy: 0.8375\n",
            "Epoch 4/10, Loss: 0.4437, Test Accuracy: 0.8555\n",
            "Epoch 5/10, Loss: 0.4263, Test Accuracy: 0.8588\n",
            "Epoch 6/10, Loss: 0.4079, Test Accuracy: 0.8437\n",
            "Epoch 7/10, Loss: 0.3951, Test Accuracy: 0.8644\n",
            "Epoch 8/10, Loss: 0.3847, Test Accuracy: 0.8600\n",
            "Epoch 9/10, Loss: 0.3786, Test Accuracy: 0.8689\n",
            "Epoch 10/10, Loss: 0.3704, Test Accuracy: 0.8705\n"
          ]
        }
      ],
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
        "model = DropoutMLP(**hparams)\n",
        "\n",
        "# write your training and testing code here\n",
        "# Loading the FashionMNIST dataset\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Defining training and evaluation functions\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Setting device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training setup\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=hparams['lr'])\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    test_accuracy = evaluate_model(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54985a45",
      "metadata": {
        "origin_pos": 29,
        "id": "54985a45"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool for avoiding overfitting. Often tools are used jointly.\n",
        "Note that dropout is\n",
        "used only during training:\n",
        "it replaces an activation $h$ with a random variable with expected value $h$.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "### 1. What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load FashionMNIST dataset\n",
        "def load_data(batch_size=64):\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Model definition\n",
        "def initialize_model(dropout_1, dropout_2):\n",
        "    hparams = {\n",
        "        'num_outputs': 10, 'num_hiddens_1': 256, 'num_hiddens_2': 256,\n",
        "        'dropout_1': dropout_1, 'dropout_2': dropout_2, 'lr': 0.1\n",
        "    }\n",
        "    model = DropoutMLP(**hparams)\n",
        "    return model\n",
        "\n",
        "# Training and Evaluation Functions\n",
        "def train_and_evaluate(model, train_loader, test_loader, num_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        test_accuracy = evaluate_model(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Loading data\n",
        "train_loader, test_loader = load_data()\n",
        "\n",
        "# Experiment 1: dropout_1 = 0.3, dropout_2 = 0.5\n",
        "print(\"Experiment 1: dropout_1 = 0.3, dropout_2 = 0.5\")\n",
        "model1 = initialize_model(dropout_1=0.3, dropout_2=0.5)\n",
        "train_and_evaluate(model1, train_loader, test_loader, num_epochs=10)\n",
        "\n",
        "# Experiment 2: dropout_1 = 0.5, dropout_2 = 0.3\n",
        "print(\"\\nExperiment 2: dropout_1 = 0.5, dropout_2 = 0.3\")\n",
        "model2 = initialize_model(dropout_1=0.5, dropout_2=0.3)\n",
        "train_and_evaluate(model2, train_loader, test_loader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rrHmOQcm92Z",
        "outputId": "bb608677-07d7-4fd4-c805-a76da012cd30"
      },
      "id": "8rrHmOQcm92Z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1: dropout_1 = 0.3, dropout_2 = 0.5\n",
            "Epoch 1/10, Loss: 0.7448, Test Accuracy: 0.7767\n",
            "Epoch 2/10, Loss: 0.4876, Test Accuracy: 0.8159\n",
            "Epoch 3/10, Loss: 0.4359, Test Accuracy: 0.8500\n",
            "Epoch 4/10, Loss: 0.4097, Test Accuracy: 0.8624\n",
            "Epoch 5/10, Loss: 0.3859, Test Accuracy: 0.8407\n",
            "Epoch 6/10, Loss: 0.3742, Test Accuracy: 0.8579\n",
            "Epoch 7/10, Loss: 0.3570, Test Accuracy: 0.8652\n",
            "Epoch 8/10, Loss: 0.3505, Test Accuracy: 0.8636\n",
            "Epoch 9/10, Loss: 0.3391, Test Accuracy: 0.8746\n",
            "Epoch 10/10, Loss: 0.3300, Test Accuracy: 0.8724\n",
            "\n",
            "Experiment 2: dropout_1 = 0.5, dropout_2 = 0.3\n",
            "Epoch 1/10, Loss: 0.7717, Test Accuracy: 0.7878\n",
            "Epoch 2/10, Loss: 0.5030, Test Accuracy: 0.8290\n",
            "Epoch 3/10, Loss: 0.4585, Test Accuracy: 0.8260\n",
            "Epoch 4/10, Loss: 0.4294, Test Accuracy: 0.8341\n",
            "Epoch 5/10, Loss: 0.4104, Test Accuracy: 0.8561\n",
            "Epoch 6/10, Loss: 0.3942, Test Accuracy: 0.8455\n",
            "Epoch 7/10, Loss: 0.3810, Test Accuracy: 0.8598\n",
            "Epoch 8/10, Loss: 0.3738, Test Accuracy: 0.8586\n",
            "Epoch 9/10, Loss: 0.3646, Test Accuracy: 0.8629\n",
            "Epoch 10/10, Loss: 0.3563, Test Accuracy: 0.8524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**:\n",
        "- **Experiment 1** (`dropout_1 = 0.3`, `dropout_2 = 0.5`):\n",
        "  - The model started with a test accuracy of 77.67% and gradually improved to 87.24% after 10 epochs.\n",
        "  - The final loss stabilized around 0.3300, indicating that the model is able to generalize well with this dropout configuration.\n",
        "\n",
        "- **Experiment 2** (`dropout_1 = 0.5`, `dropout_2 = 0.3`):\n",
        "  - The model started with a test accuracy of 78.78% and reached 85.24% after 10 epochs.\n",
        "  - The final loss was around 0.3563, which is slightly higher than Experiment 1.\n",
        "\n",
        "**Qualitative Takeaways**:\n",
        "- **Lower dropout in initial layers** (Experiment 1) resulted in better test accuracy and lower loss compared to **higher dropout in the initial layers** (Experiment 2).\n",
        "- Dropout closer to the input layer (Experiment 2) may disrupt the feature extraction process more, which could explain the slightly lower performance. In contrast, a higher dropout probability in later layers (Experiment 1) seems to improve generalization by focusing regularization towards the deeper layers where co-adaptation of features is more likely to occur.\n",
        "  \n",
        "This suggests that placing a higher dropout rate in deeper layers might be preferable for achieving better generalization."
      ],
      "metadata": {
        "id": "ECWYo5DLxkgx"
      },
      "id": "ECWYo5DLxkgx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Increase the number of epochs to 50 and compare the results."
      ],
      "metadata": {
        "id": "uc_fm-k5x_3p"
      },
      "id": "uc_fm-k5x_3p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: Training for 50 epochs with dropout_1 = 0.5, dropout_2 = 0.5\n",
        "print(\"\\nExperiment 3: Training for 50 epochs with dropout_1 = 0.5, dropout_2 = 0.5\")\n",
        "model3 = initialize_model(dropout_1=0.5, dropout_2=0.5)\n",
        "train_and_evaluate(model3, train_loader, test_loader, num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQOdKSHUnBe_",
        "outputId": "b4a87bde-561b-4894-f3e7-bc57f068e327"
      },
      "id": "GQOdKSHUnBe_",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3: Training for 50 epochs with dropout_1 = 0.5, dropout_2 = 0.5\n",
            "Epoch 1/50, Loss: 0.7935, Test Accuracy: 0.8043\n",
            "Epoch 2/50, Loss: 0.5233, Test Accuracy: 0.8261\n",
            "Epoch 3/50, Loss: 0.4726, Test Accuracy: 0.8419\n",
            "Epoch 4/50, Loss: 0.4456, Test Accuracy: 0.8440\n",
            "Epoch 5/50, Loss: 0.4218, Test Accuracy: 0.8513\n",
            "Epoch 6/50, Loss: 0.4058, Test Accuracy: 0.8535\n",
            "Epoch 7/50, Loss: 0.3961, Test Accuracy: 0.8549\n",
            "Epoch 8/50, Loss: 0.3856, Test Accuracy: 0.8632\n",
            "Epoch 9/50, Loss: 0.3770, Test Accuracy: 0.8650\n",
            "Epoch 10/50, Loss: 0.3689, Test Accuracy: 0.8693\n",
            "Epoch 11/50, Loss: 0.3591, Test Accuracy: 0.8674\n",
            "Epoch 12/50, Loss: 0.3546, Test Accuracy: 0.8699\n",
            "Epoch 13/50, Loss: 0.3516, Test Accuracy: 0.8688\n",
            "Epoch 14/50, Loss: 0.3481, Test Accuracy: 0.8612\n",
            "Epoch 15/50, Loss: 0.3398, Test Accuracy: 0.8772\n",
            "Epoch 16/50, Loss: 0.3336, Test Accuracy: 0.8782\n",
            "Epoch 17/50, Loss: 0.3299, Test Accuracy: 0.8722\n",
            "Epoch 18/50, Loss: 0.3300, Test Accuracy: 0.8806\n",
            "Epoch 19/50, Loss: 0.3267, Test Accuracy: 0.8758\n",
            "Epoch 20/50, Loss: 0.3210, Test Accuracy: 0.8722\n",
            "Epoch 21/50, Loss: 0.3158, Test Accuracy: 0.8701\n",
            "Epoch 22/50, Loss: 0.3140, Test Accuracy: 0.8786\n",
            "Epoch 23/50, Loss: 0.3109, Test Accuracy: 0.8782\n",
            "Epoch 24/50, Loss: 0.3092, Test Accuracy: 0.8788\n",
            "Epoch 25/50, Loss: 0.3067, Test Accuracy: 0.8795\n",
            "Epoch 26/50, Loss: 0.3054, Test Accuracy: 0.8753\n",
            "Epoch 27/50, Loss: 0.3015, Test Accuracy: 0.8755\n",
            "Epoch 28/50, Loss: 0.2976, Test Accuracy: 0.8842\n",
            "Epoch 29/50, Loss: 0.2997, Test Accuracy: 0.8702\n",
            "Epoch 30/50, Loss: 0.2950, Test Accuracy: 0.8786\n",
            "Epoch 31/50, Loss: 0.2909, Test Accuracy: 0.8867\n",
            "Epoch 32/50, Loss: 0.2906, Test Accuracy: 0.8830\n",
            "Epoch 33/50, Loss: 0.2890, Test Accuracy: 0.8902\n",
            "Epoch 34/50, Loss: 0.2871, Test Accuracy: 0.8843\n",
            "Epoch 35/50, Loss: 0.2864, Test Accuracy: 0.8830\n",
            "Epoch 36/50, Loss: 0.2805, Test Accuracy: 0.8885\n",
            "Epoch 37/50, Loss: 0.2823, Test Accuracy: 0.8865\n",
            "Epoch 38/50, Loss: 0.2811, Test Accuracy: 0.8834\n",
            "Epoch 39/50, Loss: 0.2806, Test Accuracy: 0.8851\n",
            "Epoch 40/50, Loss: 0.2789, Test Accuracy: 0.8901\n",
            "Epoch 41/50, Loss: 0.2739, Test Accuracy: 0.8871\n",
            "Epoch 42/50, Loss: 0.2721, Test Accuracy: 0.8883\n",
            "Epoch 43/50, Loss: 0.2722, Test Accuracy: 0.8877\n",
            "Epoch 44/50, Loss: 0.2705, Test Accuracy: 0.8848\n",
            "Epoch 45/50, Loss: 0.2722, Test Accuracy: 0.8893\n",
            "Epoch 46/50, Loss: 0.2683, Test Accuracy: 0.8824\n",
            "Epoch 47/50, Loss: 0.2680, Test Accuracy: 0.8858\n",
            "Epoch 48/50, Loss: 0.2679, Test Accuracy: 0.8919\n",
            "Epoch 49/50, Loss: 0.2647, Test Accuracy: 0.8872\n",
            "Epoch 50/50, Loss: 0.2614, Test Accuracy: 0.8908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**:\n",
        "- Training the model for 50 epochs with `dropout_1 = 0.5` and `dropout_2 = 0.5` yielded a final test accuracy of approximately **89.08%** and a final loss of around **0.2614**.\n",
        "- Test accuracy initially improved rapidly, reaching around 87-88% by epoch 30, after which it continued to improve but at a slower pace.\n",
        "\n",
        "**Qualitative Takeaways**:\n",
        "- **Increased epochs** allowed the model to achieve higher accuracy as it continued learning from the data.\n",
        "- However, the rate of improvement slowed after around 30 epochs, showing **diminishing returns** in accuracy. This suggests that while longer training can yield better results, the model begins to converge after a certain number of epochs.\n",
        "- The use of dropout (with both `dropout_1` and `dropout_2` at 0.5) helped maintain the model's generalization even after prolonged training, reducing the risk of overfitting.\n"
      ],
      "metadata": {
        "id": "-1c7masKyNMn"
      },
      "id": "-1c7masKyNMn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Why Dropout Is Not Typically Used at Test Time\n",
        "\n",
        "**Answer**:\n",
        "Dropout is used only during training because it introduces randomness by randomly deactivating neurons in each layer, which helps prevent overfitting by forcing the model to learn redundant representations. However, at test time, we want to use the **full model without any randomness** to ensure stable and consistent predictions.\n",
        "\n",
        "If dropout were applied during testing, the predictions would vary due to random deactivation of neurons, resulting in **inconsistent and less reliable predictions**. Instead, during testing, we use the entire network with all neurons active, and each neuron’s weights are implicitly scaled to account for the dropout used during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "1. **Changing Dropout Rates**: Using a lower dropout rate in earlier layers and a higher rate in later layers improves generalization and accuracy.\n",
        "2. **Increasing Epochs**: Training for more epochs can enhance accuracy but with diminishing improvements after a certain point.\n",
        "3. **Dropout at Test Time**: Dropout is not used at test time to maintain stability and consistency in predictions."
      ],
      "metadata": {
        "id": "1SRJUAO-ySAV"
      },
      "id": "1SRJUAO-ySAV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}